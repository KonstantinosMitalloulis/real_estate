Hallo!

Dieses Projekt hat die Daten von der Website www.immowelt.de mittels Web Scraping gesammelt. Das Projekt fokussiert sich auf Wohnungen und Häuser in Deutschland, die zum Verkauf stehen. Alle verwendeten Services (Postgres, Airflow, Metabase, pgAdmin) sind innerhalb von Docker Compose enthalten, und beim ersten Docker Compose up werden die Ordner dags, logs und plugins erstellt. Es wurden zwei DAGs programmiert. Der erste dient zum Initialen Laden der Datenbank. Er fasst alle initialen CSVs zusammen, transformiert sie und importiert sie am Ende in die Datenbank. Die nach dem ersten Laden erstellte Datenbank folgt dem Star Schema, also enthält sie eine Fact Table und viele Dimension Tables. Der zweite DAG dient zum Aktualisieren der Datenbank durch Web Scraping auf der Website von Immowelt. Beide DAGs werden von Airflow orchestriert. Am Ende beider DAGs wird eine Nachricht über einen Discord-Kanal geschickt, die informiert, dass der DAG erfolgreich ausgeführt wurde. Im Ordner csvs gibt es drei weitere Unterordner, nämlich back_up_csvs, initial_csvs und temporary_csvs. In back_up_csvs werden Back-up-Dateien abgelegt, die mit dem jeweiligen Update zu tun haben, und in temporary_csvs Dateien, die während des Update-Laufs zum Austausch von Daten zwischen den Tasks des DAGs dienen. Der Ordner temporary_csvs wird nach jedem Update geleert. In initial_csvs sind die für den ersten DAG relevanten web-scraped Dateien zu finden, die Rohdaten enthalten.


